{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='notebook_header'><b>CS 309 - Robot Learning</b></p>\n",
    "<p class='notebook_header'>Homework 1 - Adam Howard and Branden Heng</p>\n",
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 1: Linear Algebra</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following matrix/vector functions using NumPy operations.\n",
    "\n",
    "If the function's operation isn't possible for matrix or vector inputs, return None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "def multiply(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "def divide(a, b):\n",
    "    return None\n",
    "\n",
    "def transpose(a):\n",
    "    return a.T\n",
    "\n",
    "def two_norm(a):\n",
    "    return np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using your code from above, solve the following equations. If an operation isn't possible, put None or comment with \"Not Possible\".</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "u = \\begin{bmatrix} 2 \\\\ 3 \\\\ 9 \\end{bmatrix}, \\:\n",
    "v = \\begin{bmatrix} -2 \\\\ 1 \\\\ 8 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([\n",
    "    [2],\n",
    "    [3],\n",
    "    [9]\n",
    "])\n",
    "\n",
    "v = np.array([\n",
    "    [-2],\n",
    "    [1],\n",
    "    [8]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u + v = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ u - v = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ u * v = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ u \\div v = \\begin{bmatrix} ? \\end{bmatrix} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0]\n",
      " [ 4]\n",
      " [17]]\n",
      "\n",
      "[[4]\n",
      " [2]\n",
      " [1]]\n",
      "\n",
      "[[71]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE - Completed\n",
    "\n",
    "u_plus_v = add(u, v)\n",
    "print(u_plus_v, end = '\\n\\n')\n",
    "\n",
    "u_minus_v = subtract(u, v)\n",
    "print(u_minus_v, end = '\\n\\n')\n",
    "\n",
    "u_mult_v = multiply(u.T, v)\n",
    "print(u_mult_v)\n",
    "\n",
    "# This operation is not possible\n",
    "u_div_v = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ u^{\\;T} * v = \\; ? $$  \n",
    "\n",
    "$$ u * v^{\\;T} = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ u^{\\;T} * u = \\; ? $$  \n",
    "\n",
    "$$ \\left \\| u \\right \\|_{2}^{2} = ? $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71]]\n",
      "\n",
      "[[ -4   2  16]\n",
      " [ -6   3  24]\n",
      " [-18   9  72]]\n",
      "\n",
      "[[94]]\n",
      "\n",
      "9.695359714832659\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE - Completed\n",
    "\n",
    "u_transpose_v = multiply(u.T, v)\n",
    "print(u_transpose_v, end = '\\n\\n')\n",
    "\n",
    "u_v_transpose = np.outer(u, v.T)\n",
    "print(u_v_transpose, end = '\\n\\n')\n",
    "\n",
    "u_transpose_u = multiply(u.T, u)\n",
    "print(u_transpose_u, end = '\\n\\n')\n",
    "\n",
    "two_norm_u = two_norm(u)\n",
    "print(two_norm_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A = \\begin{bmatrix} 1 & 6 & 5\\\\ 0 & -4 & -1\\\\ 7 & 2 & 3 \\end{bmatrix}, \\: \n",
    "B = \\begin{bmatrix} 3 & 1 & 1\\\\ 4 & -1 & 7\\\\ 7 & 0 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A + B = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ A - B = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ A * B = \\begin{bmatrix} ? \\end{bmatrix} $$  \n",
    "\n",
    "$$ A \\div B = \\begin{bmatrix} ? \\end{bmatrix} $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  7  6]\n",
      " [ 4 -5  6]\n",
      " [14  2  3]]\n",
      "\n",
      "[[-2  5  4]\n",
      " [-4 -3 -8]\n",
      " [ 0  2  3]]\n",
      "\n",
      "[[ 62  -5  43]\n",
      " [-23   4 -28]\n",
      " [ 50   5  21]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE - Completed\n",
    "\n",
    "A = np.array([[1, 6, 5],\n",
    "              [0, -4, -1],\n",
    "              [7, 2, 3]])\n",
    "\n",
    "B = np.array([[3, 1, 1],\n",
    "              [4, -1, 7],\n",
    "              [7, 0, 0]])\n",
    "\n",
    "a_plus_b = add(A, B)\n",
    "print(a_plus_b, end = '\\n\\n')\n",
    "\n",
    "a_minus_b = subtract(A, B)\n",
    "print(a_minus_b, end = '\\n\\n')\n",
    "\n",
    "a_mult_b = multiply(A, B)\n",
    "print(a_mult_b)\n",
    "\n",
    "# This operation is not possible\n",
    "a_div_b = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C = \\begin{bmatrix} 5 & 1 \\\\ -1 & 7 \\\\ 3 & 0 \\end{bmatrix}\n",
    "$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.array([[5, 1],\n",
    "              [-1, 7],\n",
    "              [3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right Pseudo Inverse of C:\n",
    "$$ \\begin{bmatrix} 0 & -0.0625 & 0.5 \\\\ 0.0625 & 0.125 & 0.125 \\end{bmatrix} $$  \n",
    " \n",
    "\n",
    "Left Pseudo Inverse of C:\n",
    "$$ \\begin{bmatrix} 0.1443299 & -0.02061856 & 0.08591065 \\\\ 0.0257732 & 0.13917526 & 0.00343643 \\end{bmatrix} $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.     -0.0625  0.5   ]\n",
      " [ 0.0625  0.125   0.125 ]]\n",
      "\n",
      "[[ 0.1443299  -0.02061856  0.08591065]\n",
      " [ 0.0257732   0.13917526  0.00343643]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE - Completed\n",
    "\n",
    "right_pinv = multiply(c.T, np.linalg.inv(multiply(c, c.T)))\n",
    "print(right_pinv, end = '\\n\\n')\n",
    "\n",
    "left_pinv = multiply(np.linalg.inv(multiply(c.T, c)), c.T)\n",
    "print(left_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class='section_header'><b>Part 2: Regression</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ordinary Least Squares below. \n",
    "\n",
    "$$ \\Theta = (x^{T}x)^{-1} * x^{T} * y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** Ordinary Least Squares in terms of what it optimizes.\n",
    "\n",
    "**Explanation:** The purpose of OLS is to draw a regression line through a collection of data that results in the smallest sum of squared residuals. This value can be found for each data point by finding the y-distance from the data point to the regression line, and squaring it. The line created that has the lowest sum (the addition of the described operation performed on each data point) is the optimal line for the data based on this regression method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Don't change this cell!\n",
    "# Load in the data about the study on students\n",
    "train = np.loadtxt('train.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = train.T\n",
    "X_train = np.array([x_0, x_1, x_2]).T\n",
    "Y_train = np.expand_dims(y, 1)\n",
    "\n",
    "test = np.loadtxt('test.csv', delimiter=',')\n",
    "x_0, x_1, x_2, y = test.T\n",
    "X_test = np.array([x_0, x_1, x_2]).T\n",
    "Y_test = np.expand_dims(y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was an imaginary study done on 101 students at Crest University. The study surveyed students for the amount they have spent on electronics, books, pencils, and food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the **amount students spend on electronics ($Y$)** is linearly related to the **amount they spend on books ($ X_{0} $), pencils ($ X_{1}$)**, and **food ($ X_{2}$)**, \n",
    "**implement** the Ordinary Least Squares method to model this regression problem.\n",
    "\n",
    "The data is read in from the previous cell code. **X_train** has the input features, while **Y_train** has corresponding target outputs.\n",
    "\n",
    "After finding a solution, try to measure the error between your predictions and the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3884.63897009]\n",
      " [-9717.11198125]\n",
      " [ 1943.82567058]]\n",
      "\n",
      "[[-38358.89260064]\n",
      " [-95902.72468824]\n",
      " [ 19180.94784402]]\n",
      "\n",
      "22.945401691675993\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for OLS here. DO NOT use any other libraries in your first implementation.\n",
    "\n",
    "def OLS(X, y):\n",
    "    theta = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, y))\n",
    "    return theta\n",
    "\n",
    "# TODO: check against training data\n",
    "theta_0 = OLS(X_train, Y_train)\n",
    "print(theta_0, end = '\\n\\n')\n",
    "\n",
    "# TODO: test your model on testing data\n",
    "theta_1 = OLS(X_test, Y_test)\n",
    "print(theta_1, end = '\\n\\n')\n",
    "Y_pred = np.dot(X_test, theta_1)\n",
    "result = np.square(np.subtract(Y_test, Y_pred)).mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** what collinearity is.\n",
    "\n",
    "**Explanation:** Collinearity is a term that describes two independent variables in a regression function that are correlated. This means that they are closely related to one another, as through a functional relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write** the equation for Ridge Regression below.\n",
    "$$ \\Theta = ((\\Phi^{T} \\Phi) + (\\lambda I))^{-1} * (\\Phi^{T} y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the purpose of ridge regression and its advantages and disadvantages over OLS.\n",
    "\n",
    "**Explanation:** The most significant issue with OLS regression is that the regression line is very susceptible to being significantly skewed by data outliers. What ridge regression does is 'punish' the outlying data points in order to create a more accurate regression model. OLS also has issues with multicollinearity also that are accounted for in ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** your regression model with ridge regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.92519992]\n",
      " [-0.7015006 ]\n",
      " [ 0.54357729]]\n",
      "\n",
      "[[ 1.80989722]\n",
      " [-0.97587051]\n",
      " [ 0.59920797]]\n",
      "\n",
      "23.495653452967773\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Ridge Regression here. DO NOT use any other libraries.\n",
    "\n",
    "def RR(X, y, ridge=0.001):\n",
    "    product = multiply(X.T, X)\n",
    "    rows = product.shape[0]\n",
    "    theta = multiply(multiply(np.linalg.inv(add(product, ridge * np.identity(rows))), X.T), y)\n",
    "    return theta\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = RR(X_train, Y_train)\n",
    "print(theta, end = '\\n\\n')\n",
    "\n",
    "# TODO: test your model on testing data\n",
    "theta = RR(X_test, Y_test)\n",
    "print(theta, end = '\\n\\n')\n",
    "Y_pred = np.dot(X_test, theta)\n",
    "result = np.square(np.subtract(Y_test, Y_pred)).mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences ridge regression created for theta compared to OLS, and why these differences even exist. Also try different values for the ridge parameters and describe how they affect your results.\n",
    "\n",
    "**Explanation:** Ridge regression created a regression model that was much more accurate than the one that was created from OLS regression. These differences exist because ridge regression limits the influence of outlying data values whenever creating a regression model for the data. When altering the ridge variable for Ridge Regression though, it appeared that when the value was increased, the penalty term was of more effect, and when it was decreased, it lessened the effect of the punishment term in creating the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other regularizers other than ridge regression, such as LASSO. **Explain** the differences between LASSO and Ridge Regression and how it changes the solution mathematically.\n",
    "\n",
    "**Explanation:** LASSO can sometimes result in the loss of data which will lead to models that are lackluster. This is because LASSO deals in L1 regularization whereas Ridge Regression uses L2 regularization. L1 and L2 are different in terms of their punishment term. LASSO will reduce less important data completely through the L1's punishment term, which is not desired in every case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.99099792  2.01763613 -0.00368638]\n",
      "\n",
      "[2.99318387e+00 1.99720599e+00 2.22150120e-03]\n",
      "\n",
      "46239.34132635852\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for LASSO here\n",
    " \n",
    "from sklearn import linear_model\n",
    "\n",
    "def LASSO(X, y, ridge=.001):\n",
    "    line = linear_model.Lasso(alpha = ridge)\n",
    "    line.fit(X, y)\n",
    "    return line.coef_\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = LASSO(X_train, Y_train)\n",
    "print(theta, end = '\\n\\n')\n",
    "\n",
    "# TODO: check against testing data\n",
    "theta = LASSO(X_test, Y_test)\n",
    "print(theta, end = '\\n\\n')\n",
    "Y_pred = np.dot(X_test, theta)\n",
    "result = np.square(np.subtract(Y_test, Y_pred)).mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the effect elastic nets had on your values for theta compared to OLS. Also try different values for the ridge parameters and describe how they affect your results.\n",
    "\n",
    "**Explanation:** When using Elastic Net Regularizatin for the data, the mean distance of each point from the regression line seems to be much larger than when using OLS. Whenever the ridge value was changed for the Elastic Nets method, it hardly changed the mean y-distance of each data point from the regression line, but it appears that the mean y-distance decreased as the ridge value increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.79648533 41.67033897  5.17022768]\n",
      "\n",
      "[24.79648533 41.67033897  5.17022768]\n",
      "\n",
      "422224957.31703436\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create code for Elastic Nets here. You can use a library such as scipy\n",
    "# Code borrowed from Scipy page: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "    \n",
    "def EN(X,y):\n",
    "    X, y = make_regression(n_features = 3, random_state = 0)\n",
    "    regr = ElasticNet(random_state = 0)\n",
    "    regr.fit(X, y)\n",
    "    return regr.coef_\n",
    "\n",
    "# TODO: check against training data\n",
    "theta = EN(X_train, Y_train)\n",
    "print(theta, end = '\\n\\n')\n",
    "\n",
    "# TODO: check against testing data\n",
    "theta = EN(X_test, Y_test)\n",
    "print(theta, end = '\\n\\n')\n",
    "Y_pred = np.dot(X_test, theta)\n",
    "result = np.square(np.subtract(Y_test, Y_pred)).mean()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the differences between LASSO, Ridge Regression and Elastic Nets and how it changes the solution mathematically.\n",
    "\n",
    "**Explanation:** LASSO and Ridge Regression both have the possibility of underfitting or overfitting the regression line that is created for a group of data, as they are calculated with a single regularizer. With Elastic Nets, however, L1 and L2 regularization are combined, producing a model that more properly fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** the purpose of a regularizer.\n",
    "\n",
    "**Explanation:** The purpose of a regularizer in a regression function is to limit the importance of outlying data values with multicollinear data points. This 'regulation' of these outlying data points assists in the creation of a more accurate regression model. It also seeks to prevent the overfitting or underfitting of a regression model through a collection of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give two examples where a regularizer would give more robust models.\n",
    "\n",
    "**Examples:** One example where a regularizer would benefit a predictive data model is in a model representing the relationship between a country's GDP and its per capita income. As one would expect, as a country's GDP increases, its per capita income is likely to increase also, and a regularizer would limit the influence of certain countries where this trend isn't necessarily true. A second example where using a regularizer can provide a more robust model is when looking for correlations between quality of education and poverty within a region, a regularizer can be utilized to find a model that isn't overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain** with reference to the dataset why a regularizer achieved better performance than OLS.\n",
    "\n",
    "**Explanation:** A specific example that we identified in the file test.csv are students 7 and 12 who spent roughly the same amount on electronics, but differing amounts on the other items. In total, Student 7 spent roughly 1,556.2 credits on items $x_{0}, x_{1}$, and $x_{2}$, and Student 2 spent roughly 1,786.5 credits on those items. Given how the x-values differed between the two students, when creating a regression line using OLS regression, the line would most likely be a skewed representation of the data. However, when adding a regularizer, the margin of error that is present using OLS is decreased, making it a better representation of the relationship between the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement feature transformation to fit a line to the curve generated from the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('feature_transform.csv', delimiter=',')\n",
    "y = X[:,2].reshape(500, 1)\n",
    "X = X[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Write the lambda function phi which will transform X\n",
    "# TODO: Plot the transformation and the resulting line after transforming\n",
    "# Code borrowed from Ilham's Office Hours\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def phi(x, degree):\n",
    "    columns = []\n",
    "    n = len(x)\n",
    "    for i in range(degree + 1):\n",
    "        if (i == 0):\n",
    "            columns.append(np.ones(n))\n",
    "        else:\n",
    "            columns.append(x**i)\n",
    "    X = np.matrix(columns).T\n",
    "    return X\n",
    "\n",
    "X_new = phi(X[:,0], 3)\n",
    "theta = OLS(X_new, y)\n",
    "coef = np.array(theta).ravel()\n",
    "phi_plot = np.linspace(0, 1, 500)\n",
    "\n",
    "def poly_coefficients(x, coefs):\n",
    "    k = len(coefs)\n",
    "    y = 0\n",
    "    for i in range(k):\n",
    "        y += coefs[i] * (x**i)\n",
    "    return y\n",
    "\n",
    "y = poly_coefficients(phi_plot, coef)\n",
    "\n",
    "plt.plot(phi_plot,y,color=(0.0,0.5,0.0))\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr class='light-separate' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what you learned about polynomial regression and explain what is happening to the model as you increase the degrees. Run the cell below and use the slider to help you.\n",
    "\n",
    "**Explanation:** When the degree of the regression line is increased from one to ten, the test and train errors have a trend of decreasing, and this makes sense because the data is not linear. However, as the degree increases past ten, the errors siginificantly increase due to the increased importance of outlying data values when plotting the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece72f94322c40a3856b8f35c96683b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='degree', max=20, min=1), Output()), _dom_classes=('widgeâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(degree)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT ALTER\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "data = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "data = pd.concat([data,pd.Series(boston.target,name='MEDV')],axis=1)\n",
    "\n",
    "X = data[['LSTAT']].values\n",
    "y = data['MEDV']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "temp = pd.DataFrame({'x':x_train.reshape(1, 354)[0], 'y':y_train})\n",
    "temp = temp.sort_values('x')\n",
    "x_train = temp['x'].values.reshape(354,1)\n",
    "y_train = temp['y'].values\n",
    "\n",
    "temp = pd.DataFrame({'x':x_test.reshape(1, 152)[0], 'y':y_test})\n",
    "temp = temp.sort_values('x')\n",
    "x_test = temp['x'].values.reshape(152,1)\n",
    "y_test = temp['y'].values\n",
    "\n",
    "def f(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(x_train,y_train)\n",
    "    y_plot = model.predict(x_test)\n",
    "    \n",
    "    plt.scatter(x_train, y_train, s=10, color='red', alpha=.3)\n",
    "    plt.scatter(x_test, y_test, s=10)\n",
    "\n",
    "    test_sr = (y_test - y_plot)**2\n",
    "    test_ssr = test_sr.sum()\n",
    "    test_asr = test_ssr/len(test_sr)\n",
    "    \n",
    "    y_plot_train = model.predict(x_train)\n",
    "    train_sr = (y_train - y_plot_train)**2\n",
    "    train_ssr = train_sr.sum()\n",
    "    train_asr = train_ssr/len(train_sr)\n",
    "    \n",
    "    plt.plot(x_test, y_plot, label=\"degree %d\" % degree + '; Test Error: %.2f' % test_asr + '; Train Error: %.2f' % train_asr, color='green')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "interact(f, degree = widgets.IntSlider(min=1, max=20, step=1, value=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
